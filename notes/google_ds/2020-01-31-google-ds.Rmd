---
title: "Google DS interview prep"
output: 
  github_document:
    toc: true
    #highlight: breezedark
    pandoc_args: --webtex
---

```{r}
library(tidyverse)
```


Topic hierarchy for x:  

1. What is x in both technical and non-technical terms?  
2. Why is x relevant to data science, or at Google specifically? 
3. What are the assumptions of x?  
4. What is the math behind x?  
5. Can you implement x in code?  


# Collecting Data

## Types of Bias

Three types of bias in data collection:

1. **non-response bias**: a large proportion of those sampled do not participate/respond (e.g., you get a 20% completion rate on a survey)
2. **response bias**: when those that respond do not respond truthfully (e.g., they say what they think the researcher wants to hear)  
3. **selection bias**: when the sample population doesn't reflect the true population (e.g., you're studying domestic well properties, but your sample includes both domestic wells and agricultural supply wells). Another example of selection bias that relates to 1. is that the small proportion that responds to a survey may be a self-selecting group, and thus may reflect not reflect the true population. 

All forms of bias may or may not be observable. 


## Data collection strategies

Furthermore, we can sample based on probability or not:

Probability-based sampling: 

1. simple random sample: randomly sample from a population  
2. stratified random sample: sample equally from strata (e.g., sex, age, geographic region)
3. cluster sample: randomly sample clusters, (e.g., to find salary of professors at a school, randomly sample departments, and analyze salaries in those clusters)
```{r}
# table of people giving age and height
d <- tibble(age    = sample(c(20,30,40),   1000, replace = TRUE),
            height = sample(c(50,60),      1000, replace = TRUE),
            gender = sample(c("M","F"),    1000, replace = TRUE),
            group  = sample(LETTERS[1:10], 1000, replace = TRUE))

# simple random sample of 100 individuals (rows) from d
d[sample(1:1000, 100, replace = FALSE), ]

# stratified random sample: gender is the strata.
# take 50 random samples from each strata.
split(d, d$gender) %>% 
  lapply(., function(x) x[ sample(1:nrow(x), 50 , replace=FALSE) , ] )

# cluster sample: groups A:E are the cluster. 
filter(d, group %in% LETTERS[1:5]) 
```

By contrast, non-probability based methods (convenience sampling and gathering volunteers) can be easier and cheaper, but is not representative of the population and is suspect to **selection bias**. 

Probability based methods can be generalized to the population via inference. 


## Types of Studies

**Observational**: think EDA. You can draw correlations and speculate about a relationship, but can't draw cause and effect.

**Experimental**: an experimental study involves random assignment of a treatment, and can draw cause and effect.

For example, in an **observational study** we might observe a relationship between a customer being given a 20% off coupon, and how much they spend at a grocery store. In an **experimental** study, we would randomly assign coupons to customers and test for significant differences in spending between the two groups (e.g., does receiving a coupon make a customer more likely to purchase more?).


## Simpson's paradox

Relationship between variables within subgroups can be **reversed** when the subgroups are combined. 


## Variables

* **response** (dependent): what's of interest  
* **predictor** (independent): what's being used to predict  
* **lurking variable**: a predictor that's not in the model but that influences the response  
* **confounding variable**: a predictor in the model which relates to other predictors, thus impacting the relationship between variables  
  + precisely, in the association of **A** and **B**, **X** is a confounder if it is associated with **BOTH** **A** and **B**.


A lurking variable, when included in the study may be discovered to have a confounding effect. Then we'd call it a confounding variable.  


## Principles of Experimental Design

The purpose of experimental design is to ensure that effects observed in an experiment are likely the result the treatment, rather than by caused by chance.  

* **control**: control for effects due to factors other than the ones of primary interest  
* **randomization**: random distribution of subjects into groups prevents selection bias  
  + randomization ensures that the distribution of subjects into groups is not biased  
* **replication**: ensure a sufficient number of subjects per group to ensure that differences between groups are detectable
  + replication allows for estimating the uncertainty associated with the experiment due to uncontrolled variation 
  + increases precision  
* **stratification**: also called "blocking" means taking measurements at different times (e.g., morning and night) if it's anticipated that there might be differences among these periods  
* **representativeness**: are the subjects representative of the population you want to study? I.e., is the study free of bias (response, non-response, selection)?

If you can, fix a variable. If you can't, consider stratifying it. If you can't fix or stratify the variable, randomize it.


***  

# Probability

Probability of observing event A is $P(A)$, and is bounded by 0 and 1: $0 \le P(A) \le 1$.

Probability of a not observing A is the compliment, $P(A')$, also called $P(A^c) = 1 - P(A)$. 

## Set notation

Used to define the sample space $S$, the set of all possible outcomes that may occur. For example, consider the $S$ for tossing two die:

```{r}
expand.grid(1:6, 1:6)
nrow(expand.grid(1:6, 1:6)) # number of outcomes in S
```

The set for all possible combinations of two coin flips is $S = \{ HH, HT, TH, TT \}$

### Set operations

[union, intersection, compliment, disjoint](https://online.stat.psu.edu/stat500/lesson/2/2.2)  

> **Tip**: read the intersection, $\cap$ as _"AND"_.  

For the disjoint set $A \cap B = \emptyset$, $P(A \cap B) = 0$.

For the union of events that are not mutually exclusive, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. 


### Conditional probability

Dependent events: 

Probability of A given B, $P(A | B) = P(A \cap B) / P(B)$. 

Probability of B given A, $P(B | A) = P(B \cap A) / P(A)$.

Note that usually, $P(A|B) \ne P(B|A)$.


### Independent events

[Two events are independent if either of the following is true](https://online.stat.psu.edu/stat500/lesson/2/2.6):

$P(A \cap B) = P(A) \cdot P(B)$  
$P(A|B) = P(A)$ and $P(B|A) = P(B)$  


### Review

$P(A \cup B)$ = union = probability of $A$ or $B$  
$P(A \cap B)$ = intersection = probability of $A$ and $B$  
$P(A')$ = compliment = probability of NOT $A$, i.e., $P(B) + P(C) + ... P(N)$  
$P(A | B)$ = conditional = probability of $A$ given $B$

And another rule. Can re-write conditional probability (above) as:

$P(A \cap B) = P(A|B) \cdot P(B)$

Another helpful rule:  

$P(A \: or \: B) = P(A) + P(B) - P(A \cap B)$

### Bayes Theorem

$P(A|B) = \frac{P(B|A) P(A)} {P(B|A) P(A) + P(B|A')P(A')}$


***

# Probability Distributions

Below are examples of discrete (binomial) and continuous (normal) distributions.

Probability **mass** functions $\longrightarrow$ **discrete** variables, and $f(x) = P(X=x)$  

probability **density** functions $\longrightarrow$ **continuous** variables, and $f(x) \ne P(X=x)$  


## Expected value and variance of a discrete random variable

Expected value (mean) is 

$\bar{x} = E(X) = \sum x_i f(x_i)$ where $f(x_i)$ is the probability of event $x_i$, $P(X=x_i)$.  

Variance ($\sigma^2$) is $\sigma^2 = Var(X) = \sum (x_i - \bar{x})^2 f(x_i)$, also written as $\sum x_i^2 f(x_i) - \bar{x}^2$.

Standard deviation $\sigma$ is simply the root of the variance. 

```{r}
# x_i = prior convictions, n = number of prisoners
d <- tibble(xi = 0:4, n = c(80,265,100,40,15))

# probability of event x_i, f(x_i)
d$p <- d$n / sum(d$n)

# expected value = sum(x_i * f(x_i))
ev <- sum(d$xi * d$p) 
ev # 1.29 prior convictions (E doesn't need to be a count)

# variance = sum(x_i - EV) * f(x_i)
var <- sum((d$xi - ev)^2 * d$p)
var

# standard deviation 
sd <- sqrt(var)
sd
```


## Binomial random variables

### Binomial distribution

Special discrete distribution where there are two possible outcomes of a discrete random variable. 

Assumptions (requirements):  

1. $n$ identical trials  
2. Each trial has one of two outcomes (success, failure)  
3. Success probability $p$, same from trial to trial  
4. the $n$ trials are independent  

If these conditions are satisfied, the random variable $X$ = number of successes in $n$ trials is a **binomial random variable** with:  

$\bar{x} = E(X) = np$ (mean)  

$\sigma^2 = Var(X) = np(1-p)$ (variance)  

$\sigma = \sqrt(np(1-p))$ (standard deviation)  


```{r}
# consider 5 independent trials with 25% success. 
# probability of 0 successes: P(X=0)
dbinom(x=0, size=5, prob=0.25)

# probability of 4 or more successes, P(X >= 4)
dbinom(x=4, size=5, prob=0.25) + dbinom(x=5, size=5, prob=0.25)
# same as
sum( dbinom(x=4:5, size=5, prob=0.25) )

# expected value for 5 trails? E(X) = np
5 * 0.25

# standard deviation of successes in 5 trails, 
# SD = sqrt(Var), and Var = np(1 - p)
sqrt(5*0.25*0.75)
```

Consider an experiment with exactly two outcomes (e.g., flipping a coin), and we call heads success. This is a binomial result. We flip the coin 100 times (e.g., 100 independent trials, or experiments). If it's a fair coin, the probability of success (heads), $p =$ 0.5. The expected value is $np =$ 100 * 0.5 = 50. Thus, we expect a normal distribution with mean = 50. 

Here we simulate the probability mass function of a binomial distribution, under varying success probability, and thus expected value ($\mu = E(X) = np$). `size = 100` indicates 100 independent trials, or experiments ($n$). `prob = x` gives the probability of success $p$ across the independent trials, which we vary. `n = 1000` means that for each call to `rbinom` we use 1000 random draws from a binomial distribution with the given $n$ and $p$.

Under varying $p$, the probability mass function is:  

```{r}
# `rbinom` is somewhat confusing, since the n parameter refers to the 
# number of samples to draw, and the SIZE is actually the n in common
# formulations of the binomial distribution
tibble(`p = 0.10` = rbinom(n=1000, size = 100, prob = 0.10),
       `p = 0.25` = rbinom(n=1000, size = 100, prob = 0.25),
       `p = 0.50` = rbinom(n=1000, size = 100, prob = 0.50),
       `p = 0.75` = rbinom(n=1000, size = 100, prob = 0.75),
       `p = 0.90` = rbinom(n=1000, size = 100, prob = 0.90)) %>% 
  pivot_longer(cols = 1:5, names_to = "p", values_to = "val") %>% 
  ggplot(aes(val)) +
  geom_histogram(stat = "density") +
  facet_wrap(~p, scales = "free_y") +
  labs(title = "Probability mass function for binomial distribution",
       subtitle = "1,000 random draws for 100 trials, across varying success probability, p",
       x = "Trails (n)", y = "Probability (P(X = k))") 
```


### Binomial probability

The number of possible samples of size $k$ from a population of size $n$.  

For a binomial random variable:  

$P(X=k) = \left( \frac{n}{k} \right) p^k (1-p)^{(n-k)}$  

where  

$\left( \frac{n}{k} \right) = \frac{n!}{k!(n-k)!}$


## The Normal Distribution

A special case of a distribution of random variable. For a random variable $X$, $P(X = x) = 0$ unlike discrete random variables, for which $P(X = k) = \left( \frac{n}{k} \right) p^k (1-p)^{(n-k)}$. 

Whereas the probability **mass** function of a discrete binomial function is described by $P(X=k) = f(x)$, the probability **density** function of a continuous function at a single location is infinitesimally small, $P(X=X_i) = 0$, and instead is defined as the area under the curve over an interval bounded by $a$ and $b$: $P(a < X < b)$.  

Given a normal distribution with mean $\mu$ and standard deviation, $\sigma$, the z-score of a value $x$ is the difference from the mean divided by the standard deviation, $z = \frac{x-\mu}{\sigma}$. Z-scores essentially allow us to standardize any normal distribution to a standard normal distribution, which then allows us to easily calculate probabilities from known values. Thus, z-scores are useful for comparing between different distributions, and related to the **empirical rule**, also called the **68-95-99.7 rule**: $[\mu - 1\sigma : \mu + 1\sigma]$, $[\mu - 2\sigma : \mu + 2\sigma]$, $[\mu - 3\sigma : \mu + 3\sigma]$ = $P(-1 < Z < 1)$, $P(-2 < Z < 2)$, $P(-3 < Z < 3)$ = 68, 95, 99.7 percent of the area under the normal distribution. 

The max possible z score for a data set is $\frac{(n-1)}{\sqrt n}$.

The standard normal distribution has $\mu = 0$ and $\sigma = 1$, $N(0,1)$:  

```{r}
# simulate a standard normal distribution
tibble(x = seq(-4,4,by=0.1), 
       y = dnorm(seq(-4,4,by=0.1), 0, 1)) %>% 
  ggplot(aes(x, y)) +
  geom_line()
```


## t-distribution

Bell shaped continuous distribution that approaches the normal distribution with increasingly large degrees of freedom. 

```{r}
x <- seq(-10, 10, by = 0.1)
tibble(xindex = x,
       norm  = dnorm(x),
       df1   = dt(x, 1), 
       df5   = dt(x, 5), 
       df10  = dt(x, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('norm','df10', 'df5', 'df1'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(-10, 10)) +
  labs(color = "Degrees of \nFreedom")
```

## Chi-squared distribution

Right-skewed distribution that depends on the degrees of freedom.

```{r}
x <- seq(0, 30, by = 0.1)
tibble(xindex = x,
       df1   = dchisq(x, 1), 
       df5   = dchisq(x, 5), 
       df10  = dchisq(x, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('df1', 'df5', 'df10'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 20)) +
  labs(color = "Degrees of \nFreedom")
```

## F-distribution 

Right skewed distribution that depends on two parameters: the numerator and denominator degrees of freedom. 
```{r}
x <- seq(0, 10, by = 0.1)
tibble(xindex = x,
       df11   = df(x, 1, 1), 
       df15   = df(x, 1, 5), 
       df51   = df(x, 5, 1),
       df1010 = df(x, 10, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('df11', 'df15', 'df51', 'df1010'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 10)) +
  labs(color = "Degrees of \nFreedom")
```


## Sampling distribution

Sample statistics (e.g., SD, mean) of random samples are also random variables. 

A distribution of sample statistics is called a sampling distribution. 

Suppose we draw 20 samples from a population and compute their mean, $\bar x_1$. We repeat this 1,000 times, generating a distribution of $\bar x_1, \bar x_2, ... \bar x_{1000}$. This is the sampling distribution of the sample mean.  

```{r}
set.seed(7)

# create a population from the standard normal distribution
pop <- rnorm(10000)

# take 20 samples from the population, compute mean, and repeat 1000 times
x <- vector(length = 1000)

for(i in 1:1000) {
  s    <- sample(pop, 30, replace = FALSE)
  x[i] <- mean(s) 
}

tibble(s) %>% 
  ggplot(aes(s)) +
  geom_line(stat="density") +
  labs(title = "Sampling distribution of the sample mean",
       subtitle = "1,000 samples of size 30 from N(0,1)", x = "")
```

If the population is normally distributed, the sampling distribution of the sample mean is also normally distributed, no matter the sample size. The sample mean $\bar x$ has mean $\mu$ and standard deviation equal to standard error, 

$$SD(\bar X) = SE(\bar X) = \frac{\sigma}{\sqrt n}$$ 
Thus the z score of the sample mean is 

$$\frac{\bar x - \mu}{\frac{\sigma}{\sqrt n}}$$

If the sample comes from a distribution that is not normally distributed, the sample mean is still normally distributed if the sample is large, via the Central Limit Theorem. 

If the population is skewed, the sample distribution looks more and more normal when n gets larger.  

```{r}
# simulate normal population
pn <- rnorm(10000,0,1)
ggplot(tibble(pn), aes(pn)) + geom_histogram() + labs(title = "N(0,1)")

# skewed population - beta distribution
pb <- rbeta(10000, 2, 10) 
ggplot(tibble(pb), aes(pb)) + geom_histogram() + labs(title="Beta(2,10)")

# sample extremely small sample sizes from normal population
sample_dist <- function(dist, n, ...) {
  return(sapply(1:10000, function(x) mean(sample(dist, n))))
}

tibble(n2   = sample_dist(pn, n = 2),
       n100 = sample_dist(pn, 100),
       b2   = sample_dist(pb, 2),
       b100 = sample_dist(pb, 100)) %>% 
  pivot_longer(everything(), names_to = "dist", values_to = "sample_mean") %>% 
  ggplot(aes(sample_mean)) +
  geom_line(stat="density") +
  facet_wrap(~dist, scales = "free") +
  labs(subtitle = "Skewed beta population with small sample size -> skewed sampling distribution \nNormal population with small sample size -> normal sampling distribution \nLarge sample size from skewed or normal distribution -> normal sampling distribution")
```

Therefore, CLT tells us that the sampling distribution of the sample mean is normal or approximately normal if either

1. the population distribution is normal  
2. the sample size is large  

And, the sampling distribution has the same mean as the population mean $\mu$, and SD = SE = $\frac{\sigma}{\sqrt n}$.


## Sampling distribution of the sample proportion




## A/B testing

1. 
2. 
3. 
4. 
5. 


## Confidence intervals

1. 
2. 
3. 
4. 
5. 

# Statistical Learning

## Supervised v Unsupervised learning

## Bias-Variance tradeoff


# Linear Regression

## Simple linear regression

## Multiple linear regression

## Regression considerations


# Classification

## Logistic regression

## Discriminant Analysis

## KNN


# Resampling methods

## Cross-validation

## Bootstrap


# Linear model selection and Regularization

## Subset selection

## Shrinkage methods

## Dimension reduction methods

## Considerations in higher dimensions


# Beyond linearity

## Polynomial regression

## Step functions

## Basis functions

## Regression Splines

## Smoothing splines

## Local regression

## Generalized additive models (GAMs)


# Tree based methods

## Classification and regression trees (CART)

## Bagging, Random Forests, Boosting


# Support Vector Machines

## Maximal Margin Classifier

## Support Vector Classifier

## Support Vector Machines (SVMs)

## SVMs with more than 2 classes


# Unsupervised Learning

## Principal Components Analysis (PCA)

## Clustering methods





***

# Misc topics

## Spatial data analysis

1. 
2. 
3. 
4. 
5. 

## Interpolation

1. 
2. 
3. 
4. 
5. 

## MCMC

1. 
2. 
3. 
4. 
5. 


## Time series analysis

1. 
2. 
3. 
4. 
5. 



Topic hierarchy for frameworks:  

1. What is x?  
2. Why is x relevant to data science, or at Google specifically? 
3. What are some key features of x?

## bash

1. 
2. 
3. 


## Git

1. 
2. 
3. 


## R

1. 
2. 
3. 
 
 
# Additional resources

[Penn State Stat 500](https://online.stat.psu.edu/stat500)
[Karl Browman's courses](https://kbroman.org/pages/teaching.html)
[Karl Browman's Stat 371 ](https://www.biostat.wisc.edu/~kbroman/teaching/stat371/syllabus.html)


# Motivation 
sapply(1:10000, sample_dist, pn, 2),
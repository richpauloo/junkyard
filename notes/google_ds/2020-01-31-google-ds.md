Google DS interview prep
================

  - [Collecting Data](#collecting-data)
      - [Types of Bias](#types-of-bias)
      - [Data collection strategies](#data-collection-strategies)
      - [Types of Studies](#types-of-studies)
      - [Simpson’s paradox](#simpsons-paradox)
      - [Variables](#variables)
      - [Principles of Experimental
        Design](#principles-of-experimental-design)
  - [Probability](#probability)
      - [Set notation](#set-notation)
          - [Set operations](#set-operations)
          - [Conditional probability](#conditional-probability)
          - [Independent events](#independent-events)
          - [Review](#review)
          - [Bayes Theorem](#bayes-theorem)
  - [Probability Distributions](#probability-distributions)
      - [Expected value and variance of a discrete random
        variable](#expected-value-and-variance-of-a-discrete-random-variable)
      - [Binomial random variables](#binomial-random-variables)
          - [Binomial distribution](#binomial-distribution)
          - [Binomial probability](#binomial-probability)
      - [The Normal Distribution](#the-normal-distribution)
      - [t-distribution](#t-distribution)
      - [Chi-squared distribution](#chi-squared-distribution)
      - [F-distribution](#f-distribution)
      - [Sampling distribution](#sampling-distribution)
      - [Normal Approximation to the
        Binomial](#normal-approximation-to-the-binomial)
  - [Confidence intervals](#confidence-intervals)
      - [Inference](#inference)
      - [Estimation and confidence
        intervals](#estimation-and-confidence-intervals)
      - [CI for the population proportion
        (example)](#ci-for-the-population-proportion-example)
      - [t-distribution](#t-distribution-1)
  - [Hypothesis Testing](#hypothesis-testing)
      - [Power](#power)
      - [Hypothesis Testing for one-sample proportion and one-sample
        mean](#hypothesis-testing-for-one-sample-proportion-and-one-sample-mean)
  - [Comparing two polulation
    parameters](#comparing-two-polulation-parameters)
      - [A/B testing](#ab-testing)
      - [Confidence intervals](#confidence-intervals-1)
  - [Statistical Learning](#statistical-learning)
      - [Supervised v Unsupervised
        learning](#supervised-v-unsupervised-learning)
      - [Bias-Variance tradeoff](#bias-variance-tradeoff)
  - [Linear Regression](#linear-regression)
      - [Simple linear regression](#simple-linear-regression)
      - [Multiple linear regression](#multiple-linear-regression)
      - [Regression considerations](#regression-considerations)
  - [Classification](#classification)
      - [Logistic regression](#logistic-regression)
      - [Discriminant Analysis](#discriminant-analysis)
      - [KNN](#knn)
  - [Resampling methods](#resampling-methods)
      - [Cross-validation](#cross-validation)
      - [Bootstrap](#bootstrap)
  - [Linear model selection and
    Regularization](#linear-model-selection-and-regularization)
      - [Subset selection](#subset-selection)
      - [Shrinkage methods](#shrinkage-methods)
      - [Dimension reduction methods](#dimension-reduction-methods)
      - [Considerations in higher
        dimensions](#considerations-in-higher-dimensions)
  - [Beyond linearity](#beyond-linearity)
      - [Polynomial regression](#polynomial-regression)
      - [Step functions](#step-functions)
      - [Basis functions](#basis-functions)
      - [Regression Splines](#regression-splines)
      - [Smoothing splines](#smoothing-splines)
      - [Local regression](#local-regression)
      - [Generalized additive models
        (GAMs)](#generalized-additive-models-gams)
  - [Tree based methods](#tree-based-methods)
      - [Classification and regression trees
        (CART)](#classification-and-regression-trees-cart)
      - [Bagging, Random Forests,
        Boosting](#bagging-random-forests-boosting)
  - [Support Vector Machines](#support-vector-machines)
      - [Maximal Margin Classifier](#maximal-margin-classifier)
      - [Support Vector Classifier](#support-vector-classifier)
      - [Support Vector Machines (SVMs)](#support-vector-machines-svms)
      - [SVMs with more than 2 classes](#svms-with-more-than-2-classes)
  - [Unsupervised Learning](#unsupervised-learning)
      - [Principal Components Analysis
        (PCA)](#principal-components-analysis-pca)
      - [Clustering methods](#clustering-methods)
  - [Misc topics](#misc-topics)
      - [Spatial data analysis](#spatial-data-analysis)
      - [Interpolation](#interpolation)
      - [MCMC](#mcmc)
      - [Time series analysis](#time-series-analysis)
      - [bash](#bash)
      - [Git](#git)
      - [R](#r)
  - [Additional resources](#additional-resources)
  - [Motivation](#motivation)

``` r
library(tidyverse)
```

    ## Registered S3 methods overwritten by 'ggplot2':
    ##   method         from 
    ##   [.quosures     rlang
    ##   c.quosures     rlang
    ##   print.quosures rlang

    ## Registered S3 method overwritten by 'rvest':
    ##   method            from
    ##   read_xml.response xml2

    ## ── Attaching packages ─────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.1.1     ✔ purrr   0.3.3
    ## ✔ tibble  2.1.1     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

Topic hierarchy for x:

1.  What is x in both technical and non-technical terms?  
2.  Why is x relevant to data science, or at Google specifically?
3.  What are the assumptions of x?  
4.  What is the math behind x?  
5.  Can you implement x in code?

# Collecting Data

## Types of Bias

Three types of bias in data collection:

1.  **non-response bias**: a large proportion of those sampled do not
    participate/respond (e.g., you get a 20% completion rate on a
    survey)
2.  **response bias**: when those that respond do not respond truthfully
    (e.g., they say what they think the researcher wants to hear)  
3.  **selection bias**: when the sample population doesn’t reflect the
    true population (e.g., you’re studying domestic well properties, but
    your sample includes both domestic wells and agricultural supply
    wells). Another example of selection bias that relates to 1. is that
    the small proportion that responds to a survey may be a
    self-selecting group, and thus may reflect not reflect the true
    population.

All forms of bias may or may not be observable.

## Data collection strategies

Furthermore, we can sample based on probability or not:

Probability-based sampling:

1.  simple random sample: randomly sample from a population  
2.  stratified random sample: sample equally from strata (e.g., sex,
    age, geographic region)
3.  cluster sample: randomly sample clusters, (e.g., to find salary of
    professors at a school, randomly sample departments, and analyze
    salaries in those clusters)

<!-- end list -->

``` r
# table of people giving age and height
d <- tibble(age    = sample(c(20,30,40),   1000, replace = TRUE),
            height = sample(c(50,60),      1000, replace = TRUE),
            gender = sample(c("M","F"),    1000, replace = TRUE),
            group  = sample(LETTERS[1:10], 1000, replace = TRUE))

# simple random sample of 100 individuals (rows) from d
d[sample(1:1000, 100, replace = FALSE), ]
```

    ## # A tibble: 100 x 4
    ##      age height gender group
    ##    <dbl>  <dbl> <chr>  <chr>
    ##  1    20     50 F      J    
    ##  2    40     60 M      A    
    ##  3    40     60 M      C    
    ##  4    30     50 M      D    
    ##  5    30     50 M      B    
    ##  6    40     50 M      C    
    ##  7    40     60 M      H    
    ##  8    20     50 F      G    
    ##  9    20     50 F      C    
    ## 10    40     50 F      I    
    ## # … with 90 more rows

``` r
# stratified random sample: gender is the strata.
# take 50 random samples from each strata.
split(d, d$gender) %>% 
  lapply(., function(x) x[ sample(1:nrow(x), 50 , replace=FALSE) , ] )
```

    ## $F
    ## # A tibble: 50 x 4
    ##      age height gender group
    ##    <dbl>  <dbl> <chr>  <chr>
    ##  1    30     60 F      I    
    ##  2    20     50 F      I    
    ##  3    20     50 F      J    
    ##  4    30     50 F      I    
    ##  5    20     60 F      G    
    ##  6    30     60 F      F    
    ##  7    40     50 F      E    
    ##  8    30     50 F      F    
    ##  9    20     60 F      J    
    ## 10    40     50 F      J    
    ## # … with 40 more rows
    ## 
    ## $M
    ## # A tibble: 50 x 4
    ##      age height gender group
    ##    <dbl>  <dbl> <chr>  <chr>
    ##  1    20     60 M      I    
    ##  2    40     50 M      G    
    ##  3    30     60 M      D    
    ##  4    40     50 M      G    
    ##  5    40     50 M      I    
    ##  6    20     60 M      H    
    ##  7    30     50 M      B    
    ##  8    40     50 M      A    
    ##  9    20     60 M      F    
    ## 10    30     50 M      C    
    ## # … with 40 more rows

``` r
# cluster sample: groups A:E are the cluster. 
filter(d, group %in% LETTERS[1:5]) 
```

    ## # A tibble: 478 x 4
    ##      age height gender group
    ##    <dbl>  <dbl> <chr>  <chr>
    ##  1    20     50 F      A    
    ##  2    40     50 F      A    
    ##  3    30     60 M      B    
    ##  4    40     50 F      D    
    ##  5    30     50 M      D    
    ##  6    30     50 M      B    
    ##  7    20     50 F      B    
    ##  8    20     60 M      B    
    ##  9    30     60 M      D    
    ## 10    40     60 M      D    
    ## # … with 468 more rows

By contrast, non-probability based methods (convenience sampling and
gathering volunteers) can be easier and cheaper, but is not
representative of the population and is suspect to **selection bias**.

Probability based methods can be generalized to the population via
inference.

## Types of Studies

**Observational**: think EDA. You can draw correlations and speculate
about a relationship, but can’t draw cause and effect.

**Experimental**: an experimental study involves random assignment of a
treatment, and can draw cause and effect.

For example, in an **observational study** we might observe a
relationship between a customer being given a 20% off coupon, and how
much they spend at a grocery store. In an **experimental** study, we
would randomly assign coupons to customers and test for significant
differences in spending between the two groups (e.g., does receiving a
coupon make a customer more likely to purchase more?).

## Simpson’s paradox

Relationship between variables within subgroups can be **reversed** when
the subgroups are combined.

## Variables

  - **response** (dependent): what’s of interest  
  - **predictor** (independent): what’s being used to predict  
  - **lurking variable**: a predictor that’s not in the model but that
    influences the response  
  - **confounding variable**: a predictor in the model which relates to
    other predictors, thus impacting the relationship between variables
      - precisely, in the association of **A** and **B**, **X** is a
        confounder if it is associated with **BOTH** **A** and **B**.

A lurking variable, when included in the study may be discovered to have
a confounding effect. Then we’d call it a confounding variable.

## Principles of Experimental Design

The purpose of experimental design is to ensure that effects observed in
an experiment are likely the result the treatment, rather than by caused
by chance.

  - **control**: control for effects due to factors other than the ones
    of primary interest  
  - **randomization**: random distribution of subjects into groups
    prevents selection bias
      - randomization ensures that the distribution of subjects into
        groups is not biased  
  - **replication**: ensure a sufficient number of subjects per group to
    ensure that differences between groups are detectable
      - replication allows for estimating the uncertainty associated
        with the experiment due to uncontrolled variation
      - increases precision  
  - **stratification**: also called “blocking” means taking measurements
    at different times (e.g., morning and night) if it’s anticipated
    that there might be differences among these periods  
  - **representativeness**: are the subjects representative of the
    population you want to study? I.e., is the study free of bias
    (response, non-response, selection)?

If you can, fix a variable. If you can’t, consider stratifying it. If
you can’t fix or stratify the variable, randomize it.

-----

# Probability

Probability of observing event A is
![P(A)](https://latex.codecogs.com/png.latex?P%28A%29 "P(A)"), and is
bounded by 0 and 1: ![0 \\le P(A)
\\le 1](https://latex.codecogs.com/png.latex?0%20%5Cle%20P%28A%29%20%5Cle%201
"0 \\le P(A) \\le 1").

Probability of a not observing A is the compliment,
![P(A')](https://latex.codecogs.com/png.latex?P%28A%27%29 "P(A')"), also
called ![P(A^c) = 1 -
P(A)](https://latex.codecogs.com/png.latex?P%28A%5Ec%29%20%3D%201%20-%20P%28A%29
"P(A^c) = 1 - P(A)").

## Set notation

Used to define the sample space
![S](https://latex.codecogs.com/png.latex?S "S"), the set of all
possible outcomes that may occur. For example, consider the
![S](https://latex.codecogs.com/png.latex?S "S") for tossing two die:

``` r
expand.grid(1:6, 1:6)
```

    ##    Var1 Var2
    ## 1     1    1
    ## 2     2    1
    ## 3     3    1
    ## 4     4    1
    ## 5     5    1
    ## 6     6    1
    ## 7     1    2
    ## 8     2    2
    ## 9     3    2
    ## 10    4    2
    ## 11    5    2
    ## 12    6    2
    ## 13    1    3
    ## 14    2    3
    ## 15    3    3
    ## 16    4    3
    ## 17    5    3
    ## 18    6    3
    ## 19    1    4
    ## 20    2    4
    ## 21    3    4
    ## 22    4    4
    ## 23    5    4
    ## 24    6    4
    ## 25    1    5
    ## 26    2    5
    ## 27    3    5
    ## 28    4    5
    ## 29    5    5
    ## 30    6    5
    ## 31    1    6
    ## 32    2    6
    ## 33    3    6
    ## 34    4    6
    ## 35    5    6
    ## 36    6    6

``` r
nrow(expand.grid(1:6, 1:6)) # number of outcomes in S
```

    ## [1] 36

The set for all possible combinations of two coin flips is ![S = \\{ HH,
HT, TH, TT
\\}](https://latex.codecogs.com/png.latex?S%20%3D%20%5C%7B%20HH%2C%20HT%2C%20TH%2C%20TT%20%5C%7D
"S = \\{ HH, HT, TH, TT \\}")

### Set operations

[union, intersection, compliment,
disjoint](https://online.stat.psu.edu/stat500/lesson/2/2.2)

> **Tip**: read the intersection,
> ![\\cap](https://latex.codecogs.com/png.latex?%5Ccap "\\cap") as
> *“AND”*.

For the disjoint set ![A \\cap B =
\\emptyset](https://latex.codecogs.com/png.latex?A%20%5Ccap%20B%20%3D%20%5Cemptyset
"A \\cap B = \\emptyset"), ![P(A \\cap B)
= 0](https://latex.codecogs.com/png.latex?P%28A%20%5Ccap%20B%29%20%3D%200
"P(A \\cap B) = 0").

For the union of events that are not mutually exclusive, ![P(A \\cup B)
= P(A) + P(B) - P(A \\cap
B)](https://latex.codecogs.com/png.latex?P%28A%20%5Ccup%20B%29%20%3D%20P%28A%29%20%2B%20P%28B%29%20-%20P%28A%20%5Ccap%20B%29
"P(A \\cup B) = P(A) + P(B) - P(A \\cap B)").

### Conditional probability

Dependent events:

Probability of A given B, ![P(A | B) = P(A \\cap B) /
P(B)](https://latex.codecogs.com/png.latex?P%28A%20%7C%20B%29%20%3D%20P%28A%20%5Ccap%20B%29%20%2F%20P%28B%29
"P(A | B) = P(A \\cap B) / P(B)").

Probability of B given A, ![P(B | A) = P(B \\cap A) /
P(A)](https://latex.codecogs.com/png.latex?P%28B%20%7C%20A%29%20%3D%20P%28B%20%5Ccap%20A%29%20%2F%20P%28A%29
"P(B | A) = P(B \\cap A) / P(A)").

Note that usually, ![P(A|B) \\ne
P(B|A)](https://latex.codecogs.com/png.latex?P%28A%7CB%29%20%5Cne%20P%28B%7CA%29
"P(A|B) \\ne P(B|A)").

### Independent events

[Two events are independent if either of the following is
true](https://online.stat.psu.edu/stat500/lesson/2/2.6):

![P(A \\cap B) = P(A) \\cdot
P(B)](https://latex.codecogs.com/png.latex?P%28A%20%5Ccap%20B%29%20%3D%20P%28A%29%20%5Ccdot%20P%28B%29
"P(A \\cap B) = P(A) \\cdot P(B)")  
![P(A|B) =
P(A)](https://latex.codecogs.com/png.latex?P%28A%7CB%29%20%3D%20P%28A%29
"P(A|B) = P(A)") and ![P(B|A) =
P(B)](https://latex.codecogs.com/png.latex?P%28B%7CA%29%20%3D%20P%28B%29
"P(B|A) = P(B)")

### Review

![P(A \\cup
B)](https://latex.codecogs.com/png.latex?P%28A%20%5Ccup%20B%29
"P(A \\cup B)") = union = probability of
![A](https://latex.codecogs.com/png.latex?A "A") or
![B](https://latex.codecogs.com/png.latex?B "B")  
![P(A \\cap
B)](https://latex.codecogs.com/png.latex?P%28A%20%5Ccap%20B%29
"P(A \\cap B)") = intersection = probability of
![A](https://latex.codecogs.com/png.latex?A "A") and
![B](https://latex.codecogs.com/png.latex?B "B")  
![P(A')](https://latex.codecogs.com/png.latex?P%28A%27%29 "P(A')") =
compliment = probability of NOT
![A](https://latex.codecogs.com/png.latex?A "A"), i.e., ![P(B) + P(C) +
...
P(N)](https://latex.codecogs.com/png.latex?P%28B%29%20%2B%20P%28C%29%20%2B%20...%20P%28N%29
"P(B) + P(C) + ... P(N)")  
![P(A | B)](https://latex.codecogs.com/png.latex?P%28A%20%7C%20B%29
"P(A | B)") = conditional = probability of
![A](https://latex.codecogs.com/png.latex?A "A") given
![B](https://latex.codecogs.com/png.latex?B "B")

And another rule. Can re-write conditional probability (above) as:

![P(A \\cap B) = P(A|B) \\cdot
P(B)](https://latex.codecogs.com/png.latex?P%28A%20%5Ccap%20B%29%20%3D%20P%28A%7CB%29%20%5Ccdot%20P%28B%29
"P(A \\cap B) = P(A|B) \\cdot P(B)")

Another helpful rule:

![P(A \\: or \\: B) = P(A) + P(B) - P(A \\cap
B)](https://latex.codecogs.com/png.latex?P%28A%20%5C%3A%20or%20%5C%3A%20B%29%20%3D%20P%28A%29%20%2B%20P%28B%29%20-%20P%28A%20%5Ccap%20B%29
"P(A \\: or \\: B) = P(A) + P(B) - P(A \\cap B)")

### Bayes Theorem

![P(A|B) = \\frac{P(B|A) P(A)} {P(B|A) P(A) +
P(B|A')P(A')}](https://latex.codecogs.com/png.latex?P%28A%7CB%29%20%3D%20%5Cfrac%7BP%28B%7CA%29%20P%28A%29%7D%20%7BP%28B%7CA%29%20P%28A%29%20%2B%20P%28B%7CA%27%29P%28A%27%29%7D
"P(A|B) = \\frac{P(B|A) P(A)} {P(B|A) P(A) + P(B|A')P(A')}")

-----

# Probability Distributions

Below are examples of discrete (binomial) and continuous (normal)
distributions.

Probability **mass** functions
![\\longrightarrow](https://latex.codecogs.com/png.latex?%5Clongrightarrow
"\\longrightarrow") **discrete** variables, and ![f(x) =
P(X=x)](https://latex.codecogs.com/png.latex?f%28x%29%20%3D%20P%28X%3Dx%29
"f(x) = P(X=x)")

probability **density** functions
![\\longrightarrow](https://latex.codecogs.com/png.latex?%5Clongrightarrow
"\\longrightarrow") **continuous** variables, and ![f(x) \\ne
P(X=x)](https://latex.codecogs.com/png.latex?f%28x%29%20%5Cne%20P%28X%3Dx%29
"f(x) \\ne P(X=x)")

## Expected value and variance of a discrete random variable

Expected value (mean) is

![\\bar{x} = E(X) = \\sum x\_i
f(x\_i)](https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D%20%3D%20E%28X%29%20%3D%20%5Csum%20x_i%20f%28x_i%29
"\\bar{x} = E(X) = \\sum x_i f(x_i)") where
![f(x\_i)](https://latex.codecogs.com/png.latex?f%28x_i%29 "f(x_i)") is
the probability of event
![x\_i](https://latex.codecogs.com/png.latex?x_i "x_i"),
![P(X=x\_i)](https://latex.codecogs.com/png.latex?P%28X%3Dx_i%29
"P(X=x_i)").

Variance (![\\sigma^2](https://latex.codecogs.com/png.latex?%5Csigma%5E2
"\\sigma^2")) is ![\\sigma^2 = Var(X) = \\sum (x\_i - \\bar{x})^2
f(x\_i)](https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3D%20Var%28X%29%20%3D%20%5Csum%20%28x_i%20-%20%5Cbar%7Bx%7D%29%5E2%20f%28x_i%29
"\\sigma^2 = Var(X) = \\sum (x_i - \\bar{x})^2 f(x_i)"), also written as
![\\sum x\_i^2 f(x\_i) -
\\bar{x}^2](https://latex.codecogs.com/png.latex?%5Csum%20x_i%5E2%20f%28x_i%29%20-%20%5Cbar%7Bx%7D%5E2
"\\sum x_i^2 f(x_i) - \\bar{x}^2").

Standard deviation
![\\sigma](https://latex.codecogs.com/png.latex?%5Csigma "\\sigma") is
simply the root of the variance.

``` r
# x_i = prior convictions, n = number of prisoners
d <- tibble(xi = 0:4, n = c(80,265,100,40,15))

# probability of event x_i, f(x_i)
d$p <- d$n / sum(d$n)

# expected value = sum(x_i * f(x_i))
ev <- sum(d$xi * d$p) 
ev # 1.29 prior convictions (E doesn't need to be a count)
```

    ## [1] 1.29

``` r
# variance = sum(x_i - EV) * f(x_i)
var <- sum((d$xi - ev)^2 * d$p)
var
```

    ## [1] 0.8659

``` r
# standard deviation 
sd <- sqrt(var)
sd
```

    ## [1] 0.9305375

## Binomial random variables

### Binomial distribution

Special discrete distribution where there are two possible outcomes of a
discrete random variable.

Assumptions (requirements):

1.  ![n](https://latex.codecogs.com/png.latex?n "n") identical trials  
2.  Each trial has one of two outcomes (success, failure)  
3.  Success probability ![p](https://latex.codecogs.com/png.latex?p
    "p"), same from trial to trial  
4.  the ![n](https://latex.codecogs.com/png.latex?n "n") trials are
    independent

If these conditions are satisfied, the random variable
![X](https://latex.codecogs.com/png.latex?X "X") = number of successes
in ![n](https://latex.codecogs.com/png.latex?n "n") trials is a
**binomial random variable** with:

![\\bar{x} = E(X) =
np](https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D%20%3D%20E%28X%29%20%3D%20np
"\\bar{x} = E(X) = np") (mean)

![\\sigma^2 = Var(X) =
np(1-p)](https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3D%20Var%28X%29%20%3D%20np%281-p%29
"\\sigma^2 = Var(X) = np(1-p)") (variance)

![\\sigma =
\\sqrt(np(1-p))](https://latex.codecogs.com/png.latex?%5Csigma%20%3D%20%5Csqrt%28np%281-p%29%29
"\\sigma = \\sqrt(np(1-p))") (standard deviation)

``` r
# consider 5 independent trials with 25% success. 
# probability of 0 successes: P(X=0)
dbinom(x=0, size=5, prob=0.25)
```

    ## [1] 0.2373047

``` r
# probability of 4 or more successes, P(X >= 4)
dbinom(x=4, size=5, prob=0.25) + dbinom(x=5, size=5, prob=0.25)
```

    ## [1] 0.015625

``` r
# same as
sum( dbinom(x=4:5, size=5, prob=0.25) )
```

    ## [1] 0.015625

``` r
# expected value for 5 trails? E(X) = np
5 * 0.25
```

    ## [1] 1.25

``` r
# standard deviation of successes in 5 trails, 
# SD = sqrt(Var), and Var = np(1 - p)
sqrt(5*0.25*0.75)
```

    ## [1] 0.9682458

Consider an experiment with exactly two outcomes (e.g., flipping a
coin), and we call heads success. This is a binomial result. We flip the
coin 100 times (e.g., 100 independent trials, or experiments). If it’s a
fair coin, the probability of success (heads), ![p
=](https://latex.codecogs.com/png.latex?p%20%3D "p =") 0.5. The expected
value is ![np =](https://latex.codecogs.com/png.latex?np%20%3D "np =")
100 \* 0.5 = 50. Thus, we expect a normal distribution with mean = 50.

Here we simulate the probability mass function of a binomial
distribution, under varying success probability, and thus expected value
(![\\mu = E(X) =
np](https://latex.codecogs.com/png.latex?%5Cmu%20%3D%20E%28X%29%20%3D%20np
"\\mu = E(X) = np")). `size = 100` indicates 100 independent trials, or
experiments (![n](https://latex.codecogs.com/png.latex?n "n")). `prob =
x` gives the probability of success
![p](https://latex.codecogs.com/png.latex?p "p") across the independent
trials, which we vary. `n = 1000` means that for each call to `rbinom`
we use 1000 random draws from a binomial distribution with the given
![n](https://latex.codecogs.com/png.latex?n "n") and
![p](https://latex.codecogs.com/png.latex?p "p").

Under varying ![p](https://latex.codecogs.com/png.latex?p "p"), the
probability mass function is:

``` r
# `rbinom` is somewhat confusing, since the n parameter refers to the 
# number of samples to draw, and the SIZE is actually the n in common
# formulations of the binomial distribution
tibble(`p = 0.10` = rbinom(n=1000, size = 100, prob = 0.10),
       `p = 0.25` = rbinom(n=1000, size = 100, prob = 0.25),
       `p = 0.50` = rbinom(n=1000, size = 100, prob = 0.50),
       `p = 0.75` = rbinom(n=1000, size = 100, prob = 0.75),
       `p = 0.90` = rbinom(n=1000, size = 100, prob = 0.90)) %>% 
  pivot_longer(cols = 1:5, names_to = "p", values_to = "val") %>% 
  ggplot(aes(val)) +
  geom_histogram(stat = "density") +
  facet_wrap(~p, scales = "free_y") +
  labs(title = "Probability mass function for binomial distribution",
       subtitle = "1,000 random draws for 100 trials, across varying success probability, p",
       x = "Trails (n)", y = "Probability (P(X = k))") 
```

    ## Warning: Ignoring unknown parameters: binwidth, bins, pad

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

### Binomial probability

The number of possible samples of size
![k](https://latex.codecogs.com/png.latex?k "k") from a population of
size ![n](https://latex.codecogs.com/png.latex?n "n").

For a binomial random variable:

![P(X=k) = \\left( \\frac{n}{k} \\right) p^k
(1-p)^{(n-k)}](https://latex.codecogs.com/png.latex?P%28X%3Dk%29%20%3D%20%5Cleft%28%20%5Cfrac%7Bn%7D%7Bk%7D%20%5Cright%29%20p%5Ek%20%281-p%29%5E%7B%28n-k%29%7D
"P(X=k) = \\left( \\frac{n}{k} \\right) p^k (1-p)^{(n-k)}")

where

![\\left( \\frac{n}{k} \\right) =
\\frac{n\!}{k\!(n-k)\!}](https://latex.codecogs.com/png.latex?%5Cleft%28%20%5Cfrac%7Bn%7D%7Bk%7D%20%5Cright%29%20%3D%20%5Cfrac%7Bn%21%7D%7Bk%21%28n-k%29%21%7D
"\\left( \\frac{n}{k} \\right) = \\frac{n!}{k!(n-k)!}")

## The Normal Distribution

A special case of a distribution of random variable. For a random
variable ![X](https://latex.codecogs.com/png.latex?X "X"), ![P(X = x)
= 0](https://latex.codecogs.com/png.latex?P%28X%20%3D%20x%29%20%3D%200
"P(X = x) = 0") unlike discrete random variables, for which ![P(X = k) =
\\left( \\frac{n}{k} \\right) p^k
(1-p)^{(n-k)}](https://latex.codecogs.com/png.latex?P%28X%20%3D%20k%29%20%3D%20%5Cleft%28%20%5Cfrac%7Bn%7D%7Bk%7D%20%5Cright%29%20p%5Ek%20%281-p%29%5E%7B%28n-k%29%7D
"P(X = k) = \\left( \\frac{n}{k} \\right) p^k (1-p)^{(n-k)}").

Whereas the probability **mass** function of a discrete binomial
function is described by ![P(X=k) =
f(x)](https://latex.codecogs.com/png.latex?P%28X%3Dk%29%20%3D%20f%28x%29
"P(X=k) = f(x)"), the probability **density** function of a continuous
function at a single location is infinitesimally small, ![P(X=X\_i)
= 0](https://latex.codecogs.com/png.latex?P%28X%3DX_i%29%20%3D%200
"P(X=X_i) = 0"), and instead is defined as the area under the curve over
an interval bounded by ![a](https://latex.codecogs.com/png.latex?a "a")
and ![b](https://latex.codecogs.com/png.latex?b "b"): ![P(a \< X \<
b)](https://latex.codecogs.com/png.latex?P%28a%20%3C%20X%20%3C%20b%29
"P(a \< X \< b)").

Given a normal distribution with mean
![\\mu](https://latex.codecogs.com/png.latex?%5Cmu "\\mu") and standard
deviation, ![\\sigma](https://latex.codecogs.com/png.latex?%5Csigma
"\\sigma"), the z-score of a value
![x](https://latex.codecogs.com/png.latex?x "x") is the difference from
the mean divided by the standard deviation, ![z =
\\frac{x-\\mu}{\\sigma}](https://latex.codecogs.com/png.latex?z%20%3D%20%5Cfrac%7Bx-%5Cmu%7D%7B%5Csigma%7D
"z = \\frac{x-\\mu}{\\sigma}"). Z-scores essentially allow us to
standardize any normal distribution to a standard normal distribution,
which then allows us to easily calculate probabilities from known
values. Thus, z-scores are useful for comparing between different
distributions, and related to the **empirical rule**, also called the
**68-95-99.7 rule**: ![\[\\mu - 1\\sigma : \\mu
+ 1\\sigma\]](https://latex.codecogs.com/png.latex?%5B%5Cmu%20-%201%5Csigma%20%3A%20%5Cmu%20%2B%201%5Csigma%5D
"[\\mu - 1\\sigma : \\mu + 1\\sigma]"), ![\[\\mu - 2\\sigma : \\mu
+ 2\\sigma\]](https://latex.codecogs.com/png.latex?%5B%5Cmu%20-%202%5Csigma%20%3A%20%5Cmu%20%2B%202%5Csigma%5D
"[\\mu - 2\\sigma : \\mu + 2\\sigma]"), ![\[\\mu - 3\\sigma : \\mu
+ 3\\sigma\]](https://latex.codecogs.com/png.latex?%5B%5Cmu%20-%203%5Csigma%20%3A%20%5Cmu%20%2B%203%5Csigma%5D
"[\\mu - 3\\sigma : \\mu + 3\\sigma]") = ![P(-1 \< Z
\< 1)](https://latex.codecogs.com/png.latex?P%28-1%20%3C%20Z%20%3C%201%29
"P(-1 \< Z \< 1)"), ![P(-2 \< Z
\< 2)](https://latex.codecogs.com/png.latex?P%28-2%20%3C%20Z%20%3C%202%29
"P(-2 \< Z \< 2)"), ![P(-3 \< Z
\< 3)](https://latex.codecogs.com/png.latex?P%28-3%20%3C%20Z%20%3C%203%29
"P(-3 \< Z \< 3)") = 68, 95, 99.7 percent of the area under the normal
distribution.

The max possible z score for a data set is ![\\frac{(n-1)}{\\sqrt
n}](https://latex.codecogs.com/png.latex?%5Cfrac%7B%28n-1%29%7D%7B%5Csqrt%20n%7D
"\\frac{(n-1)}{\\sqrt n}").

The standard normal distribution has ![\\mu
= 0](https://latex.codecogs.com/png.latex?%5Cmu%20%3D%200 "\\mu = 0")
and ![\\sigma
= 1](https://latex.codecogs.com/png.latex?%5Csigma%20%3D%201
"\\sigma = 1"),
![N(0,1)](https://latex.codecogs.com/png.latex?N%280%2C1%29 "N(0,1)"):

``` r
# simulate a standard normal distribution
tibble(x = seq(-4,4,by=0.1), 
       y = dnorm(seq(-4,4,by=0.1), 0, 1)) %>% 
  ggplot(aes(x, y)) +
  geom_line()
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

## t-distribution

Bell shaped continuous distribution that approaches the normal
distribution with increasingly large degrees of freedom (n-1).

``` r
x <- seq(-10, 10, by = 0.1)
tibble(xindex = x,
       norm  = dnorm(x),
       df1   = dt(x, 1), 
       df5   = dt(x, 5), 
       df10  = dt(x, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('norm','df10', 'df5', 'df1'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(-10, 10)) +
  labs(color = "Degrees of \nFreedom")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

## Chi-squared distribution

Right-skewed distribution that depends on the degrees of freedom.

``` r
x <- seq(0, 30, by = 0.1)
tibble(xindex = x,
       df1   = dchisq(x, 1), 
       df5   = dchisq(x, 5), 
       df10  = dchisq(x, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('df1', 'df5', 'df10'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 20)) +
  labs(color = "Degrees of \nFreedom")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

## F-distribution

Right skewed distribution that depends on two parameters: the numerator
and denominator degrees of freedom.

``` r
x <- seq(0, 10, by = 0.05)
tibble(xindex = x,
       df11   = df(x, 1, 1), 
       df15   = df(x, 1, 5), 
       df51   = df(x, 5, 1),
       df1010 = df(x, 10, 10)) %>% 
  pivot_longer(-xindex, names_to = "dof", values_to = "val") %>% 
  mutate(dof = factor(dof, levels = c('df11', 'df15', 'df51', 'df1010'))) %>% 
  ggplot(aes(xindex, val, color = dof)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 10)) +
  labs(color = "Degrees of \nFreedom")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

## Sampling distribution

Sample statistics (e.g., SD, mean) of random samples are also random
variables.

A distribution of sample statistics is called a sampling distribution.

Suppose we draw 20 samples from a population and compute their mean,
![\\bar x\_1](https://latex.codecogs.com/png.latex?%5Cbar%20x_1
"\\bar x_1"). We repeat this 1,000 times, generating a distribution of
![\\bar x\_1, \\bar x\_2, ... \\bar
x\_{1000}](https://latex.codecogs.com/png.latex?%5Cbar%20x_1%2C%20%5Cbar%20x_2%2C%20...%20%5Cbar%20x_%7B1000%7D
"\\bar x_1, \\bar x_2, ... \\bar x_{1000}"). This is the sampling
distribution of the sample mean.

``` r
set.seed(7)

# create a population from the standard normal distribution
pop <- rnorm(10000)

# take 20 samples from the population, compute mean, and repeat 1000 times
x <- vector(length = 1000)

for(i in 1:1000) {
  s    <- sample(pop, 30, replace = FALSE)
  x[i] <- mean(s) 
}

tibble(s) %>% 
  ggplot(aes(s)) +
  geom_line(stat="density") +
  labs(title = "Sampling distribution of the sample mean",
       subtitle = "1,000 samples of size 30 from N(0,1)", x = "")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

If the population is normally distributed, the sampling distribution of
the sample mean is also normally distributed, no matter the sample size.
The sample mean ![\\bar
x](https://latex.codecogs.com/png.latex?%5Cbar%20x "\\bar x") has mean
![\\mu](https://latex.codecogs.com/png.latex?%5Cmu "\\mu") and standard
deviation equal to standard error,

  
![SD(\\bar X) = SE(\\bar X) = \\frac{\\sigma}{\\sqrt
n}](https://latex.codecogs.com/png.latex?SD%28%5Cbar%20X%29%20%3D%20SE%28%5Cbar%20X%29%20%3D%20%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%20n%7D
"SD(\\bar X) = SE(\\bar X) = \\frac{\\sigma}{\\sqrt n}")  
Thus the z score of the sample mean is

  
![\\frac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt
n}}](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cbar%20x%20-%20%5Cmu%7D%7B%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%20n%7D%7D
"\\frac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt n}}")  

If the sample comes from a distribution that is not normally
distributed, the sample mean is still normally distributed if the sample
is large, via the Central Limit Theorem.

If the population is skewed, the sample distribution looks more and more
normal when n gets larger.

``` r
# simulate normal population
pn <- rnorm(10000,0,1)
ggplot(tibble(pn), aes(pn)) + geom_histogram() + labs(title = "N(0,1)")
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

``` r
# skewed population - beta distribution
pb <- rbeta(10000, 2, 10) 
ggplot(tibble(pb), aes(pb)) + geom_histogram() + labs(title="Beta(2,10)")
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-12-2.png)<!-- -->

``` r
# sample extremely small sample sizes from normal population
sample_dist <- function(dist, n, ...) {
  return(sapply(1:10000, function(x) mean(sample(dist, n))))
}

tibble(n2   = sample_dist(pn, n = 2),
       n100 = sample_dist(pn, 100),
       b2   = sample_dist(pb, 2),
       b100 = sample_dist(pb, 100)) %>% 
  pivot_longer(everything(), names_to = "dist", values_to = "sample_mean") %>% 
  ggplot(aes(sample_mean)) +
  geom_line(stat="density") +
  facet_wrap(~dist, scales = "free") +
  labs(subtitle = "Skewed beta population with small sample size -> skewed sampling distribution \nNormal population with small sample size -> normal sampling distribution \nLarge sample size from skewed or normal distribution -> normal sampling distribution")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-12-3.png)<!-- -->

Therefore, CLT tells us that the sampling distribution of the sample
mean is normal or approximately normal if either

1.  the population distribution is normal  
2.  the sample size is large

And, the sampling distribution has the same mean as the population mean
![\\mu](https://latex.codecogs.com/png.latex?%5Cmu "\\mu"), and SE =
![\\frac{\\sigma}{\\sqrt
n}](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%20n%7D
"\\frac{\\sigma}{\\sqrt n}"). When the population SE
![\\sigma](https://latex.codecogs.com/png.latex?%5Csigma "\\sigma") is
unknown, we can estimate it with the sample SE
![s](https://latex.codecogs.com/png.latex?s "s").

## Normal Approximation to the Binomial

Can apply the CLT to find the sampling distribution of the sample
proportion,
![\\hat{p}](https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D
"\\hat{p}").

Consider a Bernoulli random variable
![Y](https://latex.codecogs.com/png.latex?Y "Y"):

  
![&#10;f(y) = \\begin{cases}&#10; 1, \\: success\\\\&#10; 0, \\:
failure&#10;\\end{cases}&#10;](https://latex.codecogs.com/png.latex?%0Af%28y%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%201%2C%20%5C%3A%20%20success%5C%5C%0A%20%200%2C%20%5C%3A%20failure%0A%5Cend%7Bcases%7D%0A
"
f(y) = \\begin{cases}
  1, \\:  success\\\\
  0, \\: failure
\\end{cases}
")  
and ![p](https://latex.codecogs.com/png.latex?p "p") is the probability
of success.

The Bernoulli random variable is a special case of the binomial random
variable, where the number of trials = 1. There are
![n](https://latex.codecogs.com/png.latex?n "n") values of
![Y](https://latex.codecogs.com/png.latex?Y "Y") from ![Y\_1, ...
Y\_n](https://latex.codecogs.com/png.latex?Y_1%2C%20...%20Y_n
"Y_1, ... Y_n").

Their sum ![X](https://latex.codecogs.com/png.latex?X "X") is ![X =
\\sum\_{i=1}^n
Y\_i](https://latex.codecogs.com/png.latex?X%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20Y_i
"X = \\sum_{i=1}^n Y_i"), a binomial random variable with parameters
![n](https://latex.codecogs.com/png.latex?n "n") and
![p](https://latex.codecogs.com/png.latex?p "p").

The sample proportion ![\\hat{p} =
\\frac{X}{n}](https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D%20%3D%20%5Cfrac%7BX%7D%7Bn%7D
"\\hat{p} = \\frac{X}{n}"), thus the CLT applies for large samples.

Mean of ![\\hat{p} =
p](https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D%20%3D%20p
"\\hat{p} = p"). SD = SE =
![\\sqrt{\\frac{p(1-p)}{n}}](https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Cfrac%7Bp%281-p%29%7D%7Bn%7D%7D
"\\sqrt{\\frac{p(1-p)}{n}}").

Requirements (either of):

1.  ![np \\ge 5](https://latex.codecogs.com/png.latex?np%20%5Cge%205
    "np \\ge 5")  
2.  ![n(1-p)
    \\ge 5](https://latex.codecogs.com/png.latex?n%281-p%29%20%5Cge%205
    "n(1-p) \\ge 5")

# Confidence intervals

In real life, population parameters are rarely known, but methods exist
to deal with this.

## Inference

You have data, but not data from the entire population. Inference gives
probability statements about the population of interest based on that
set of data.

**Estimation**: use sample information to estimate or predict parameter
of interest.

  - point estimates - one parameter, e.g., sample mean or sample
    proportion  
  - interval estimates - e.g., confidence interval which is likely to
    contain the true parameter of interest.

**Statistical (hypothesis) Tests**: use sample information to test the
truth of a hypothesis.

## Estimation and confidence intervals

General form of confidence interval = sample statistic
![\\pm](https://latex.codecogs.com/png.latex?%5Cpm "\\pm") margin of
error,

where margin of error = ![M \\cdot
\\hat{SE}(estimate)](https://latex.codecogs.com/png.latex?M%20%5Ccdot%20%5Chat%7BSE%7D%28estimate%29
"M \\cdot \\hat{SE}(estimate)"), and the multiplier M depends on the
level of confidence.

## CI for the population proportion (example)

First, check conditions: ![n \\hat{p}
\> 5](https://latex.codecogs.com/png.latex?n%20%5Chat%7Bp%7D%20%3E%205
"n \\hat{p} \> 5") and ![n(1-\\hat{p})
\> 5](https://latex.codecogs.com/png.latex?n%281-%5Chat%7Bp%7D%29%20%3E%205
"n(1-\\hat{p}) \> 5")? This ensures the sampling distribution is
approximately normal.

Next, use the general form. Z multiplier times the Standard Error.
Recall the familiar 1.96
![\\pm](https://latex.codecogs.com/png.latex?%5Cpm "\\pm") SE for a 95%
CI.

  
![&#10;\\hat{p} \\pm z\_{\\alpha / 2}
\\sqrt{\\frac{\\hat{p}(1-p)}{n}}&#10;](https://latex.codecogs.com/png.latex?%0A%5Chat%7Bp%7D%20%5Cpm%20z_%7B%5Calpha%20%2F%202%7D%20%5Csqrt%7B%5Cfrac%7B%5Chat%7Bp%7D%281-p%29%7D%7Bn%7D%7D%0A
"
\\hat{p} \\pm z_{\\alpha / 2} \\sqrt{\\frac{\\hat{p}(1-p)}{n}}
")  

where ![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha
"\\alpha") is typically 0.1 or 0.05, corresponding to the 90% and 95%
CIs. Note that the square root term is simply the SE of the estimate.

**A common misinterpretation of CIs** is that there is a 95%
*probability* of observing an event within the CI. This misses the
random nature of the CI which derived from a random sample.

**CORRECT INTERPRETATION** of the CI: We are 95% *confident* that the
true \[statistic of interest\] falls between the interval from x to y.
We are *confident* in the *method* that gives us this specific interval,
and if we were to repeat this method many times, we would find that
around 95% of the generated CIs would contain the true \[statistic of
interest\].

## t-distribution

The t-distribution approaches the z distribution as ![n \\rightarrow
\\infty](https://latex.codecogs.com/png.latex?n%20%5Crightarrow%20%5Cinfty
"n \\rightarrow \\infty"), are different for different degrees of
rfeedom (DOF), and like the z distribution are ccentered at 0.

``` r
x <- seq(-10,10,0.01)
tibble(
  x = x,
  `1` = dt(x, df = 1),
  `2` = dt(x, df = 10),
  `20` = dt(x, df = 100),
  `200` = dt(x, df = 500),
  `N(0,1)` = dnorm(x)
) %>% 
  pivot_longer(-x, names_to = "dof", values_to = "y") %>% 
  ggplot(aes(x, y, color = dof)) +
  geom_line() +
  labs(title = "Student's t-distribution",
       subtitle = "Increasingly normal with increasing degrees of freedom = (n-1)",
       color ="Degrees of \nfreedom \n(n-1)", x="",y="Density")
```

![](2020-01-31-google-ds_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

# Hypothesis Testing

Steps:

1.  set up hypotheses  
2.  confirm if it’s testable with a normal distribution  
3.  choose significance level (tolerance to Type I error)  
4.  calculate test statistic  
5.  convert to probability of observing test statistic or more extreme
    (p-value)  
6.  decide whether nor not to reject null hypothesis  
7.  interpret

-----

Population, Intervention, Comparison, Outcome, Time = **PICOT**.
Remember **PICOT** when defining your hypotheses.

-----

Type I error: reject ![H\_0](https://latex.codecogs.com/png.latex?H_0
"H_0") when it is TRUE

  - ![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha "\\alpha")
    (significance level) is the probability of committing a Type I
    error.

Type II error: fail to reject
![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0") when
![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0") is FALSE

  - ![\\beta](https://latex.codecogs.com/png.latex?%5Cbeta "\\beta") is
    the probability of committing a Type II error.

**Power** = ![1 -
\\beta](https://latex.codecogs.com/png.latex?1%20-%20%5Cbeta
"1 - \\beta") = is the probability
![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0") is rejected when
it is FALSE

Example:

![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0"): building is not
safe  
![H\_a](https://latex.codecogs.com/png.latex?H_a "H_a") : building is
safe

Type I error: reject ![H\_0](https://latex.codecogs.com/png.latex?H_0
"H_0") when it’s TRUE. The building is not safe, but we incorrectly
determined it was safe.

Type II error: fail to reject
![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0") when it is
FALSE. The building is safe, but we incorrectly determined it was not
safe.

**test statistic**: the sample statistic one uses to either reject
![H\_0](https://latex.codecogs.com/png.latex?H_0 "H_0") or not.

**p-value**: Commonly misinterpreted as “95% probability that this
answer is correct.” The correct interpretation of a p-value is: *the
probability of obtaining a particular test statistic or a value more
extreme, assuming the null is true.* In other words, the p-value itself
IS A RANDOM VARIABLE (with a uniform distribution under the null
hypothesis and a left skewed distribution under the alternative). If we
were to repeat the experiment many times, we would find that, if the
null was true, the probability of obtaining our test statistic or a
value more extreme is equal to the p-value.

Interesting relationship to [effect
size](https://livefreeordichotomize.com/2018/08/21/p-value-thoughts-a-twitter-follow-up/),
and [null intervals (rather than point
nulls)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5863943/pdf/pone.0188299.pdf).

p-values are easily visualized with the rejection region approach (with
a critical value, e.g.,
![t\_{0.05}](https://latex.codecogs.com/png.latex?t_%7B0.05%7D
"t_{0.05}")).

-----

statistical significance (passing a test) v practice significance
(applicable in the real world)

## Power

![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha "\\alpha") is
the probability of committing a Type I error \[0,1\]  
![\\beta](https://latex.codecogs.com/png.latex?%5Cbeta "\\beta") is the
probability of committing a Type II error \[0,1\]  
![Power = 1 -
\\beta](https://latex.codecogs.com/png.latex?Power%20%3D%201%20-%20%5Cbeta
"Power = 1 - \\beta")

Need to increase sample size for
![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha "\\alpha") and
![\\beta](https://latex.codecogs.com/png.latex?%5Cbeta "\\beta") to
decrease, and therefore for power to increase. At a fixed sample size,
decreasing ![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha
"\\alpha") increases
![\\beta](https://latex.codecogs.com/png.latex?%5Cbeta "\\beta").

Think of power as the probability \[0,1\] of not making a Type II error
(![1-\\beta](https://latex.codecogs.com/png.latex?1-%5Cbeta
"1-\\beta")).

Ways to increase power:

  - increase ![\\alpha](https://latex.codecogs.com/png.latex?%5Calpha
    "\\alpha"), although this increases P(Type I error)  
  - increase ![n](https://latex.codecogs.com/png.latex?n "n"), which
    should, for example, shrink the sample distributions about their
    mean  
  - decrease ![\\sigma](https://latex.codecogs.com/png.latex?%5Csigma
    "\\sigma") of the populations (not possible)  
  - decrease effect size you’re trying to detect (usually not possible)

## Hypothesis Testing for one-sample proportion and one-sample mean

[Penn State notes](https://online.stat.psu.edu/stat500/lesson/6a/6a.3).

# Comparing two polulation parameters

Consider 2 populations. Under certain conditions, ![n\_ip\_i \> 5 \\:
\\& \\: n\_i(1-p\_i)
\>5](https://latex.codecogs.com/png.latex?n_ip_i%20%3E%205%20%5C%3A%20%5C%26%20%5C%3A%20n_i%281-p_i%29%20%3E5
"n_ip_i \> 5 \\: \\& \\: n_i(1-p_i) \>5") for ![i
= 1, 2](https://latex.codecogs.com/png.latex?i%20%3D%201%2C%202
"i = 1, 2"), the sampling proportions
![\\hat{p\_i}](https://latex.codecogs.com/png.latex?%5Chat%7Bp_i%7D
"\\hat{p_i}"), and thus the sampling distribution of the difference in
proportions ![\\hat{p\_1} -
\\hat{p\_2}](https://latex.codecogs.com/png.latex?%5Chat%7Bp_1%7D%20-%20%5Chat%7Bp_2%7D
"\\hat{p_1} - \\hat{p_2}") approximate the normal distribution.

Then the mean is:

  
![\\hat{p\_1} -
\\hat{p\_2}](https://latex.codecogs.com/png.latex?%5Chat%7Bp_1%7D%20-%20%5Chat%7Bp_2%7D
"\\hat{p_1} - \\hat{p_2}")  
  
and the CI is:

  
![&#10;\\hat{p\_1} - \\hat{p\_2} \\: \\pm \\: \\sqrt{\\frac{\\hat{p\_1}
(1 - \\hat{p\_2})}{n\_1} + \\frac{\\hat{p\_2} (1 -
\\hat{p\_2})}{n\_2}}&#10;](https://latex.codecogs.com/png.latex?%0A%5Chat%7Bp_1%7D%20-%20%5Chat%7Bp_2%7D%20%5C%3A%20%5Cpm%20%5C%3A%20%5Csqrt%7B%5Cfrac%7B%5Chat%7Bp_1%7D%20%281%20-%20%5Chat%7Bp_2%7D%29%7D%7Bn_1%7D%20%2B%20%5Cfrac%7B%5Chat%7Bp_2%7D%20%281%20-%20%5Chat%7Bp_2%7D%29%7D%7Bn_2%7D%7D%0A
"
\\hat{p_1} - \\hat{p_2} \\: \\pm \\: \\sqrt{\\frac{\\hat{p_1} (1 - \\hat{p_2})}{n_1} + \\frac{\\hat{p_2} (1 - \\hat{p_2})}{n_2}}
")  

The test statistic is:

  
![&#10;z\* = \\frac{\\hat{p\_1}-\\hat{p\_2}
- 0}{\\sqrt{\\hat{p^\*}(1-\\hat{p^\*})(1/n\_1
+ 1/n\_2)}}&#10;](https://latex.codecogs.com/png.latex?%0Az%2A%20%3D%20%5Cfrac%7B%5Chat%7Bp_1%7D-%5Chat%7Bp_2%7D%20-%200%7D%7B%5Csqrt%7B%5Chat%7Bp%5E%2A%7D%281-%5Chat%7Bp%5E%2A%7D%29%281%2Fn_1%20%2B%201%2Fn_2%29%7D%7D%0A
"
z* = \\frac{\\hat{p_1}-\\hat{p_2} - 0}{\\sqrt{\\hat{p^*}(1-\\hat{p^*})(1/n_1 + 1/n_2)}}
")  

where ![\\hat{p^\*} =
\\frac{x\_1+x\_2}{n\_1+n\_2}](https://latex.codecogs.com/png.latex?%5Chat%7Bp%5E%2A%7D%20%3D%20%5Cfrac%7Bx_1%2Bx_2%7D%7Bn_1%2Bn_2%7D
"\\hat{p^*} = \\frac{x_1+x_2}{n_1+n_2}")

## A/B testing

1.  
2.  
3.  
4.  
5.  
## Confidence intervals

1.  
2.  
3.  
4.  
5.  
# Statistical Learning

## Supervised v Unsupervised learning

## Bias-Variance tradeoff

# Linear Regression

## Simple linear regression

## Multiple linear regression

## Regression considerations

# Classification

## Logistic regression

## Discriminant Analysis

## KNN

# Resampling methods

## Cross-validation

## Bootstrap

# Linear model selection and Regularization

## Subset selection

## Shrinkage methods

## Dimension reduction methods

## Considerations in higher dimensions

# Beyond linearity

## Polynomial regression

## Step functions

## Basis functions

## Regression Splines

## Smoothing splines

## Local regression

## Generalized additive models (GAMs)

# Tree based methods

## Classification and regression trees (CART)

## Bagging, Random Forests, Boosting

# Support Vector Machines

## Maximal Margin Classifier

## Support Vector Classifier

## Support Vector Machines (SVMs)

## SVMs with more than 2 classes

# Unsupervised Learning

## Principal Components Analysis (PCA)

## Clustering methods

-----

# Misc topics

## Spatial data analysis

1.  
2.  
3.  
4.  
5.  
## Interpolation

1.  
2.  
3.  
4.  
5.  
## MCMC

1.  
2.  
3.  
4.  
5.  
## Time series analysis

1.  
2.  
3.  
4.  
5.  
Topic hierarchy for frameworks:

1.  What is x?  
2.  Why is x relevant to data science, or at Google specifically?
3.  What are some key features of x?

## bash

1.  
2.  
3.  
## Git

1.  
2.  
3.  
## R

1.  
2.  
3.  
# Additional resources

[Penn State Stat 500](https://online.stat.psu.edu/stat500) [Karl
Browman’s courses](https://kbroman.org/pages/teaching.html) [Karl
Browman’s
Stat 371](https://www.biostat.wisc.edu/~kbroman/teaching/stat371/syllabus.html)

# Motivation

sapply(1:10000, sample\_dist, pn, 2),
